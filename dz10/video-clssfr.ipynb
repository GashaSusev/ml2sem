{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96661,"databundleVersionId":11585601,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport sys\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:48:03.381969Z","iopub.execute_input":"2025-06-23T06:48:03.382797Z","iopub.status.idle":"2025-06-23T06:48:03.387133Z","shell.execute_reply.started":"2025-06-23T06:48:03.382762Z","shell.execute_reply":"2025-06-23T06:48:03.386512Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntrain_csv_path = \"/kaggle/input/what-on-the-video/train.csv\"\ntrain_dir = \"/kaggle/input/what-on-the-video/train/\"\ntest_dir = \"/kaggle/input/what-on-the-video/test/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:48:03.388683Z","iopub.execute_input":"2025-06-23T06:48:03.388889Z","iopub.status.idle":"2025-06-23T06:48:03.404504Z","shell.execute_reply.started":"2025-06-23T06:48:03.388874Z","shell.execute_reply":"2025-06-23T06:48:03.403843Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def extract_resnet_embeddings(video_path, frame_interval=25, device='cuda'):\n    try:\n        model = models.resnet18(pretrained=True).to(device)\n        \n        model = nn.Sequential(*list(model.children())[:-1])\n        model.eval()\n        \n        preprocess = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        \n        video = cv2.VideoCapture(video_path)\n        if not video.isOpened():\n            raise ValueError(f\"Cannot open video: {video_path}\")\n        embeddings = []\n\n        frame_idx = 0\n        while True:\n            ret, frame = video.read()\n            if not ret:\n                break\n            if frame_idx % frame_interval == 0:\n                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                image = preprocess(image).unsqueeze(0).to(device)\n                with torch.no_grad():\n                    embedding = model(image)\n                    embeddings.append(embedding.cpu().view(-1))\n            frame_idx += 1\n\n        video.release()\n        if not embeddings:\n            raise ValueError(f\"No frames extracted from {video_path}\")\n        return torch.stack(embeddings)\n    except Exception as e:\n        print(f\"Error processing {video_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:48:03.405196Z","iopub.execute_input":"2025-06-23T06:48:03.405358Z","iopub.status.idle":"2025-06-23T06:48:03.418483Z","shell.execute_reply.started":"2025-06-23T06:48:03.405343Z","shell.execute_reply":"2025-06-23T06:48:03.417942Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class CNN1DLSTMClassifier(nn.Module):\n    def __init__(self, embedding_dim=512, hidden_dim=512, num_layers=2, num_classes=9, dropout=0.1):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv1d(embedding_dim, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.MaxPool1d(2)\n        )\n        self.lstm = nn.LSTM(\n            input_size=256,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        self.classifier = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.cnn(x)\n        x = x.permute(0, 2, 1)\n        _, (h_n, _) = self.lstm(x)\n        out = h_n[-1]\n        return self.classifier(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:12:14.170938Z","iopub.execute_input":"2025-06-23T07:12:14.171505Z","iopub.status.idle":"2025-06-23T07:12:14.177001Z","shell.execute_reply.started":"2025-06-23T07:12:14.171482Z","shell.execute_reply":"2025-06-23T07:12:14.176350Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, video_paths, labels, device='cpu', cache_dir='embeddings'):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.device = device\n        self.cache_dir = cache_dir\n        os.makedirs(cache_dir, exist_ok=True)\n        self.encoded_videos = self.load_embeddings()\n\n    def load_embeddings(self):\n        encoded_videos = {}\n        for video_path in tqdm(self.video_paths, desc=\"Loading embeddings\"):\n            cache_path = os.path.join(self.cache_dir, f\"{os.path.basename(video_path)}.pkl\")\n            if os.path.exists(cache_path):\n                with open(cache_path, 'rb') as f:\n                    encoded_videos[video_path] = pickle.load(f)\n            else:\n                embeddings = extract_resnet_embeddings(video_path, frame_interval=25, device=self.device)\n                if embeddings is not None:\n                    encoded_videos[video_path] = embeddings\n                    with open(cache_path, 'wb') as f:\n                        pickle.dump(embeddings, f)\n        return encoded_videos\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        embeddings = self.encoded_videos.get(video_path)\n        if embeddings is None:\n            embeddings = torch.zeros((1, 512))\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return embeddings, label\n\ndef collate_fn(batch):\n    embeddings, labels = zip(*batch)\n    padded_embeddings = nn.utils.rnn.pad_sequence(embeddings, batch_first=True)\n    labels_tensor = torch.stack(labels)\n    return padded_embeddings, labels_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:48:03.437960Z","iopub.execute_input":"2025-06-23T06:48:03.438146Z","iopub.status.idle":"2025-06-23T06:48:03.453687Z","shell.execute_reply.started":"2025-06-23T06:48:03.438132Z","shell.execute_reply":"2025-06-23T06:48:03.453035Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def compute_class_wise_accuracy(outputs, labels, threshold=0.5):\n    preds = (outputs > threshold).float()\n    correct = (preds == labels).float()\n    class_acc = correct.sum(dim=0) / labels.size(0)\n    return class_acc.mean().item(), class_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:48:03.516673Z","iopub.execute_input":"2025-06-23T06:48:03.517331Z","iopub.status.idle":"2025-06-23T06:48:03.520893Z","shell.execute_reply.started":"2025-06-23T06:48:03.517312Z","shell.execute_reply":"2025-06-23T06:48:03.520207Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\nglobal_labels = [\"animal\", \"car\", \"cloud\", \"dance\", \"fire\", \"flower\", \"food\", \"sunset\", \"water\"]\nlabel_map = {label: i for i, label in enumerate(global_labels)}\n\ntrain_data = {}\nfor _, row in train_df.iterrows():\n    video_name = os.path.basename(row['path'])\n    video_labels = row['labels'].replace(' ', '').split(',')\n    label_vec = [0] * len(global_labels)\n    for lab in video_labels:\n        if lab in label_map:\n            label_vec[label_map[lab]] = 1\n    train_data[video_name] = label_vec\n\nvideo_paths = [os.path.join(train_dir, name) for name in train_data.keys()]\nlabels_tensor = torch.tensor(list(train_data.values()), dtype=torch.float32)\n\ntrain_idx, val_idx = train_test_split(range(len(video_paths)), test_size=0.2, random_state=42)\ntrain_paths = [video_paths[i] for i in train_idx]\ntrain_labels = labels_tensor[train_idx]\nval_paths = [video_paths[i] for i in val_idx]\nval_labels = labels_tensor[val_idx]\n\ntrain_dataset = VideoDataset(train_paths, train_labels, device=device, cache_dir='embeddings_train')\nval_dataset = VideoDataset(val_paths, val_labels, device=device, cache_dir='embeddings_val')\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:12:17.934213Z","iopub.execute_input":"2025-06-23T07:12:17.934777Z","iopub.status.idle":"2025-06-23T07:12:18.009751Z","shell.execute_reply.started":"2025-06-23T07:12:17.934754Z","shell.execute_reply":"2025-06-23T07:12:18.009185Z"}},"outputs":[{"name":"stderr","text":"Loading embeddings: 100%|██████████| 232/232 [00:00<00:00, 6161.42it/s]\nLoading embeddings: 100%|██████████| 58/58 [00:00<00:00, 6023.76it/s]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"model = CNN1DLSTMClassifier(num_classes=len(global_labels), num_layers=3).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\nnum_epochs = 100\n\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n    model.train()\n    train_loss = 0\n    train_correct = torch.zeros(len(global_labels)).to(device)\n    train_total = 0\n\n    for embeddings, labels in train_loader:\n        embeddings, labels = embeddings.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(embeddings)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * labels.size(0)\n        train_total += labels.size(0)\n        class_acc, _ = compute_class_wise_accuracy(torch.sigmoid(outputs), labels)\n        train_correct += (torch.sigmoid(outputs) > 0.5).float().eq(labels).sum(dim=0)\n\n    train_loss /= train_total\n    train_acc = (train_correct / train_total).mean().item()\n\n    model.eval()\n    val_loss = 0\n    val_correct = torch.zeros(len(global_labels)).to(device)\n    val_total = 0\n    with torch.no_grad():\n        for embeddings, labels in val_loader:\n            embeddings, labels = embeddings.to(device), labels.to(device)\n            outputs = model(embeddings)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * labels.size(0)\n            val_total += labels.size(0)\n            class_acc, class_accs = compute_class_wise_accuracy(torch.sigmoid(outputs), labels)\n            val_correct += (torch.sigmoid(outputs) > 0.5).float().eq(labels).sum(dim=0)\n    \n    val_loss /= val_total\n    val_acc = (val_correct / val_total).mean().item()\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Class-Wise Accuracy: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Class-Wise Accuracy: {val_acc:.4f}\")\n        print(f\"Val Per-Class Accuracy: {', '.join([f'{global_labels[i]}: {acc:.4f}' for i, acc in enumerate(val_correct / val_total)])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:12:20.871921Z","iopub.execute_input":"2025-06-23T07:12:20.872318Z","iopub.status.idle":"2025-06-23T07:12:46.275502Z","shell.execute_reply.started":"2025-06-23T07:12:20.872290Z","shell.execute_reply":"2025-06-23T07:12:46.274771Z"}},"outputs":[{"name":"stderr","text":"Epochs:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_35/2035822184.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  label = torch.tensor(self.labels[idx], dtype=torch.float32)\nEpochs:  10%|█         | 10/100 [00:02<00:23,  3.91it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10/100:\nTrain Loss: 0.3164, Train Class-Wise Accuracy: 0.8807\nVal Loss: 0.3133, Val Class-Wise Accuracy: 0.8755\nVal Per-Class Accuracy: animal: 0.9138, car: 0.8621, cloud: 0.8276, dance: 0.9655, fire: 0.9828, flower: 0.8966, food: 0.8621, sunset: 0.8966, water: 0.6724\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  20%|██        | 20/100 [00:05<00:20,  3.98it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/100:\nTrain Loss: 0.2623, Train Class-Wise Accuracy: 0.8836\nVal Loss: 0.2866, Val Class-Wise Accuracy: 0.8851\nVal Per-Class Accuracy: animal: 0.9138, car: 0.8621, cloud: 0.8276, dance: 0.9655, fire: 0.9828, flower: 0.8966, food: 0.8621, sunset: 0.8966, water: 0.7586\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  30%|███       | 30/100 [00:07<00:17,  3.96it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 30/100:\nTrain Loss: 0.2287, Train Class-Wise Accuracy: 0.8985\nVal Loss: 0.2695, Val Class-Wise Accuracy: 0.8908\nVal Per-Class Accuracy: animal: 0.9138, car: 0.8621, cloud: 0.8276, dance: 0.9655, fire: 0.9828, flower: 0.8966, food: 0.8448, sunset: 0.8966, water: 0.8276\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  40%|████      | 40/100 [00:10<00:15,  3.96it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 40/100:\nTrain Loss: 0.1837, Train Class-Wise Accuracy: 0.9191\nVal Loss: 0.3038, Val Class-Wise Accuracy: 0.8870\nVal Per-Class Accuracy: animal: 0.9138, car: 0.8621, cloud: 0.7586, dance: 0.9655, fire: 0.9828, flower: 0.8966, food: 0.9138, sunset: 0.8276, water: 0.8621\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|█████     | 50/100 [00:12<00:12,  3.96it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 50/100:\nTrain Loss: 0.1660, Train Class-Wise Accuracy: 0.9325\nVal Loss: 0.2992, Val Class-Wise Accuracy: 0.8831\nVal Per-Class Accuracy: animal: 0.9138, car: 0.8966, cloud: 0.7586, dance: 0.9655, fire: 0.9828, flower: 0.9310, food: 0.8621, sunset: 0.7931, water: 0.8448\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  60%|██████    | 60/100 [00:15<00:10,  3.93it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 60/100:\nTrain Loss: 0.1187, Train Class-Wise Accuracy: 0.9569\nVal Loss: 0.3635, Val Class-Wise Accuracy: 0.8716\nVal Per-Class Accuracy: animal: 0.8966, car: 0.8966, cloud: 0.7759, dance: 0.9655, fire: 0.9828, flower: 0.9138, food: 0.8448, sunset: 0.7586, water: 0.8103\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  70%|███████   | 70/100 [00:17<00:07,  3.94it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 70/100:\nTrain Loss: 0.1351, Train Class-Wise Accuracy: 0.9492\nVal Loss: 0.3472, Val Class-Wise Accuracy: 0.8793\nVal Per-Class Accuracy: animal: 0.8793, car: 0.8448, cloud: 0.8276, dance: 0.9655, fire: 0.9828, flower: 0.9655, food: 0.7931, sunset: 0.8103, water: 0.8448\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  80%|████████  | 80/100 [00:20<00:05,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 80/100:\nTrain Loss: 0.2043, Train Class-Wise Accuracy: 0.9320\nVal Loss: 0.3603, Val Class-Wise Accuracy: 0.8659\nVal Per-Class Accuracy: animal: 0.9138, car: 0.8448, cloud: 0.7069, dance: 0.9655, fire: 0.9828, flower: 0.8966, food: 0.8448, sunset: 0.8448, water: 0.7931\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  90%|█████████ | 90/100 [00:22<00:02,  4.00it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 90/100:\nTrain Loss: 0.0721, Train Class-Wise Accuracy: 0.9713\nVal Loss: 0.3859, Val Class-Wise Accuracy: 0.8889\nVal Per-Class Accuracy: animal: 0.8966, car: 0.8793, cloud: 0.8621, dance: 0.9655, fire: 0.9655, flower: 0.9138, food: 0.8448, sunset: 0.8621, water: 0.8103\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|██████████| 100/100 [00:25<00:00,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 100/100:\nTrain Loss: 0.0516, Train Class-Wise Accuracy: 0.9775\nVal Loss: 0.3948, Val Class-Wise Accuracy: 0.8966\nVal Per-Class Accuracy: animal: 0.8793, car: 0.9310, cloud: 0.8448, dance: 0.9655, fire: 0.9828, flower: 0.9310, food: 0.8448, sunset: 0.8621, water: 0.8276\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/what-on-the-video/sample_submit.csv')\ntest_paths = [os.path.join(test_dir, os.path.basename(p)) for p in test_df['file_name']]\ntest_dataset = VideoDataset(test_paths, torch.zeros(len(test_paths), len(labels)), device=device, cache_dir='embeddings_test')\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n\nmodel.eval()\nall_preds = []\nwith torch.no_grad():\n    for embeddings, _ in test_loader:\n        embeddings = embeddings.to(device)\n        outputs = torch.sigmoid(model(embeddings))\n        all_preds.append(outputs.cpu())\nall_preds = torch.cat(all_preds)\npred_labels = (all_preds > 0.5).numpy()\n\nfor i in range(len(pred_labels)):\n    if np.sum(pred_labels[i]) == 0:\n        pred_labels[i][np.argmax(all_preds[i])] = True\npred_labels = [', '.join([global_labels[i] for i, p in enumerate(row) if p]) for row in pred_labels]\nsubmission = pd.DataFrame({\n    'index': test_df.index,\n    'file_name': test_df['file_name'],\n    'label': pred_labels\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:13:36.483397Z","iopub.execute_input":"2025-06-23T07:13:36.484113Z","iopub.status.idle":"2025-06-23T07:13:36.924015Z","shell.execute_reply.started":"2025-06-23T07:13:36.484088Z","shell.execute_reply":"2025-06-23T07:13:36.923311Z"}},"outputs":[{"name":"stderr","text":"Loading embeddings:   0%|          | 0/435 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nLoading embeddings: 100%|██████████| 435/435 [00:00<00:00, 1592.01it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing /kaggle/input/what-on-the-video/test/_Massachusetts Salem Witch's House with sign_preview.mp4: Cannot open video: /kaggle/input/what-on-the-video/test/_Massachusetts Salem Witch's House with sign_preview.mp4\n","output_type":"stream"},{"name":"stderr","text":"\n/tmp/ipykernel_35/2035822184.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  label = torch.tensor(self.labels[idx], dtype=torch.float32)\n","output_type":"stream"}],"execution_count":40}]}