{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96745,"databundleVersionId":11513148,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom transformers import GPT2Tokenizer\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom nltk.translate.bleu_score import sentence_bleu\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T08:18:14.103408Z","iopub.execute_input":"2025-06-23T08:18:14.103779Z","iopub.status.idle":"2025-06-23T08:18:14.109102Z","shell.execute_reply.started":"2025-06-23T08:18:14.103749Z","shell.execute_reply":"2025-06-23T08:18:14.108224Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntrain_csv_path = \"/kaggle/input/automated-video-captioning/train.csv\"\ntest_csv_path = \"/kaggle/input/automated-video-captioning/test.csv\"\ntrain_dir = \"/kaggle/input/automated-video-captioning/train_videos\"\ntest_dir = \"/kaggle/input/automated-video-captioning/test_videos\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T08:18:14.110384Z","iopub.execute_input":"2025-06-23T08:18:14.110589Z","iopub.status.idle":"2025-06-23T08:18:14.129338Z","shell.execute_reply.started":"2025-06-23T08:18:14.110572Z","shell.execute_reply":"2025-06-23T08:18:14.128588Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def extract_resnet_embeddings(video_path, frame_interval=25, device='cuda'):\n    try:\n        model = models.resnet18(pretrained=True).to(device)\n        \n        model = nn.Sequential(*list(model.children())[:-1])\n        model.eval()\n        \n        preprocess = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        \n        video = cv2.VideoCapture(video_path)\n        if not video.isOpened():\n            raise ValueError(f\"Cannot open video: {video_path}\")\n        embeddings = []\n\n        frame_idx = 0\n        while True:\n            ret, frame = video.read()\n            if not ret:\n                break\n            if frame_idx % frame_interval == 0:\n                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                image = preprocess(image).unsqueeze(0).to(device)\n                with torch.no_grad():\n                    embedding = model(image)\n                    embeddings.append(embedding.cpu().view(-1))\n            frame_idx += 1\n\n        video.release()\n        if not embeddings:\n            raise ValueError(f\"No frames extracted from {video_path}\")\n        return torch.stack(embeddings)\n    except Exception as e:\n        print(f\"Error processing {video_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T08:18:14.130192Z","iopub.execute_input":"2025-06-23T08:18:14.130438Z","iopub.status.idle":"2025-06-23T08:18:14.147452Z","shell.execute_reply.started":"2025-06-23T08:18:14.130415Z","shell.execute_reply":"2025-06-23T08:18:14.146672Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ResNetLSTMCaptioning(nn.Module):\n    def __init__(\n        self,\n        embedding_dim=512,\n        hidden_dim=256,\n        num_layers=2,\n        vocab_size=50257,\n        max_len=50,\n        dropout=0.1,\n    ):\n        super(ResNetLSTMCaptioning, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.max_len = max_len\n\n        self.video_lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n        )\n\n        self.text_embedding = nn.Embedding(vocab_size, hidden_dim)\n        \n        # Замена GRU на LSTM для текста\n        self.text_lstm = nn.LSTM(\n            input_size=hidden_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n        )\n        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, video_emb, text_ids):\n        batch_size = video_emb.size(0)\n\n        _, (video_hidden, video_cell) = self.video_lstm(video_emb)\n\n        text_emb = self.text_embedding(text_ids)\n        output, _ = self.text_lstm(text_emb, (video_hidden, video_cell))\n        return self.fc_out(output)\n\n    def generate(\n        self,\n        video_emb,\n        tokenizer,\n        max_len=50,\n        device=\"cuda\",\n    ):\n        batch_size = video_emb.size(0)\n        _, (video_hidden, video_cell) = self.video_lstm(video_emb)\n    \n        generated = torch.full(\n            (batch_size, 1), tokenizer.bos_token_id, dtype=torch.long, device=device\n        )\n        \n        for _ in range(max_len):\n            text_emb = self.text_embedding(generated[:, -1:])\n            \n            # Forward through LSTM\n            output, (video_hidden, video_cell) = self.text_lstm(\n                text_emb, (video_hidden, video_cell)\n            )\n            \n            # Get logits and next token\n            logits = self.fc_out(output[:, -1, :])\n            next_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n            \n            generated = torch.cat([generated, next_token], dim=1)\n            \n            if (next_token == tokenizer.eos_token_id).any():\n                break\n                \n        return generated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:26:44.815983Z","iopub.execute_input":"2025-06-23T09:26:44.816248Z","iopub.status.idle":"2025-06-23T09:26:44.825341Z","shell.execute_reply.started":"2025-06-23T09:26:44.816228Z","shell.execute_reply":"2025-06-23T09:26:44.824602Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def collate_fn(batch):\n    embeddings, text_ids = zip(*batch)\n    padded_embeddings = nn.utils.rnn.pad_sequence(embeddings, batch_first=True)\n    padded_text_ids = nn.utils.rnn.pad_sequence(\n        text_ids, batch_first=True, padding_value=0\n    )\n    return padded_embeddings, padded_text_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:24:05.723154Z","iopub.execute_input":"2025-06-23T09:24:05.723827Z","iopub.status.idle":"2025-06-23T09:24:05.727569Z","shell.execute_reply.started":"2025-06-23T09:24:05.723804Z","shell.execute_reply":"2025-06-23T09:24:05.726984Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class VideoCaptionDataset(Dataset):\n    def __init__(\n        self,\n        video_paths,\n        captions,\n        tokenizer,\n        device=\"cuda\",\n        cache_dir=\"embeddings\",\n        max_len=50,\n    ):\n        self.video_paths = video_paths\n        self.captions = captions\n        self.tokenizer = tokenizer\n        self.device = device\n        self.cache_dir = cache_dir\n        self.max_len = max_len\n        os.makedirs(cache_dir, exist_ok=True)\n        self.encoded_videos = self.load_embeddings()\n\n    def load_embeddings(self):\n        encoded_videos = {}\n        for video_path in tqdm(self.video_paths, desc=\"Loading embeddings\"):\n            cache_path = os.path.join(\n                self.cache_dir, f\"{os.path.basename(video_path)}.pkl\"\n            )\n            if os.path.exists(cache_path):\n                with open(cache_path, \"rb\") as f:\n                    encoded_videos[video_path] = pickle.load(f)\n            else:\n                embeddings = extract_resnet_embeddings(\n                    video_path, frame_interval=25, device=self.device\n                )\n                if embeddings is not None:\n                    encoded_videos[video_path] = embeddings\n                    with open(cache_path, \"wb\") as f:\n                        pickle.dump(embeddings, f)\n        return encoded_videos\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        embeddings = self.encoded_videos.get(video_path)\n        if embeddings is None:\n            embeddings = torch.zeros((1, 2048))\n        caption = self.captions[idx]\n        tokens = self.tokenizer(\n            caption,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_ids = tokens[\"input_ids\"].squeeze(0)\n        return embeddings, text_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:24:07.159646Z","iopub.execute_input":"2025-06-23T09:24:07.159931Z","iopub.status.idle":"2025-06-23T09:24:07.167483Z","shell.execute_reply.started":"2025-06-23T09:24:07.159909Z","shell.execute_reply":"2025-06-23T09:24:07.166741Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def compute_bleu_score(pred_texts, true_texts):\n    scores = []\n    for pred, true in zip(pred_texts, true_texts):\n        pred_tokens = pred.split()\n        true_tokens = true.split()\n        score = sentence_bleu(\n            [true_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25)\n        )\n        scores.append(score)\n    return np.mean(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:24:10.446283Z","iopub.execute_input":"2025-06-23T09:24:10.446548Z","iopub.status.idle":"2025-06-23T09:24:10.450972Z","shell.execute_reply.started":"2025-06-23T09:24:10.446529Z","shell.execute_reply":"2025-06-23T09:24:10.450229Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.bos_token_id = tokenizer.eos_token_id\n\ntrain_df = pd.read_csv(train_csv_path)\nvideo_paths = [os.path.join(train_dir, name) for name in train_df[\"file_name\"]]\ncaptions = train_df[\"caption\"].tolist()\n\ntrain_idx, val_idx = train_test_split(\n    range(len(video_paths)), test_size=0.2, random_state=42\n)\ntrain_paths = [video_paths[i] for i in train_idx]\ntrain_captions = [captions[i] for i in train_idx]\nval_paths = [video_paths[i] for i in val_idx]\nval_captions = [captions[i] for i in val_idx]\n\ntrain_dataset = VideoCaptionDataset(\n    train_paths, train_captions, tokenizer, device=device, cache_dir=\"embeddings_train\"\n)\nval_dataset = VideoCaptionDataset(\n    val_paths, val_captions, tokenizer, device=device, cache_dir=\"embeddings_val\"\n)\ntrain_loader = DataLoader(\n    train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn\n)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:24:12.544134Z","iopub.execute_input":"2025-06-23T09:24:12.544388Z","iopub.status.idle":"2025-06-23T09:24:12.919310Z","shell.execute_reply.started":"2025-06-23T09:24:12.544370Z","shell.execute_reply":"2025-06-23T09:24:12.918580Z"}},"outputs":[{"name":"stderr","text":"Loading embeddings: 100%|██████████| 482/482 [00:00<00:00, 5954.26it/s]\nLoading embeddings: 100%|██████████| 121/121 [00:00<00:00, 5510.91it/s]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"test_df = pd.read_csv(test_csv_path)\ntest_paths = [os.path.join(test_dir, name) for name in test_df[\"file_name\"]]\ntest_dataset = VideoCaptionDataset(\n    test_paths,\n    [\"\"] * len(test_paths),\n    tokenizer,\n    device=device,\n    cache_dir=\"embeddings_test\",\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:24:16.121857Z","iopub.execute_input":"2025-06-23T09:24:16.122548Z","iopub.status.idle":"2025-06-23T09:24:16.226429Z","shell.execute_reply.started":"2025-06-23T09:24:16.122522Z","shell.execute_reply":"2025-06-23T09:24:16.225786Z"}},"outputs":[{"name":"stderr","text":"Loading embeddings: 100%|██████████| 521/521 [00:00<00:00, 5939.07it/s]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"model = ResNetGRUCaptioning(\n    vocab_size=tokenizer.vocab_size, hidden_dim=256, dropout=0.2, num_layers=2\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=2e-4)\nnum_epochs = 30\n\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n    model.train()\n    train_loss = 0\n    train_total = 0\n\n    for embeddings, text_ids in train_loader:\n        embeddings, text_ids = embeddings.to(device), text_ids.to(device)\n        optimizer.zero_grad()\n        outputs = model(embeddings, text_ids[:, :-1])\n        loss = criterion(\n            outputs.view(-1, tokenizer.vocab_size), text_ids[:, 1:].reshape(-1)\n        )\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * embeddings.size(0)\n        train_total += embeddings.size(0)\n\n    train_loss /= train_total\n\n    model.eval()\n    val_loss = 0\n    val_total = 0\n    pred_captions = []\n    true_captions = val_captions\n    with torch.no_grad():\n        for embeddings, text_ids in val_loader:\n            embeddings, text_ids = embeddings.to(device), text_ids.to(device)\n            outputs = model(embeddings, text_ids[:, :-1])\n            loss = criterion(\n                outputs.view(-1, tokenizer.vocab_size), text_ids[:, 1:].reshape(-1)\n            )\n            val_loss += loss.item() * embeddings.size(0)\n            val_total += embeddings.size(0)\n\n            generated_ids = model.generate(\n                embeddings, tokenizer, max_len=50, device=device\n            )\n            for ids in generated_ids:\n                caption = tokenizer.decode(ids, skip_special_tokens=True)\n                pred_captions.append(caption)\n\n    val_loss /= val_total\n    bleu_score = compute_bleu_score(pred_captions, true_captions)\n    if (epoch+1) % 5 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, BLEU Score: {bleu_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:35:09.343460Z","iopub.execute_input":"2025-06-23T09:35:09.343776Z","iopub.status.idle":"2025-06-23T09:36:40.915567Z","shell.execute_reply.started":"2025-06-23T09:35:09.343721Z","shell.execute_reply":"2025-06-23T09:36:40.914844Z"}},"outputs":[{"name":"stderr","text":"Epochs:  17%|█▋        | 5/30 [00:15<01:15,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30:\nTrain Loss: 5.9707\nVal Loss: 6.3000, BLEU Score: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  33%|███▎      | 10/30 [00:30<01:00,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10/30:\nTrain Loss: 5.4126\nVal Loss: 5.8656, BLEU Score: 0.0012\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|█████     | 15/30 [00:45<00:45,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 15/30:\nTrain Loss: 4.8780\nVal Loss: 5.5788, BLEU Score: 0.0020\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  67%|██████▋   | 20/30 [01:00<00:30,  3.06s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/30:\nTrain Loss: 4.4417\nVal Loss: 5.4387, BLEU Score: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  83%|████████▎ | 25/30 [01:16<00:15,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 25/30:\nTrain Loss: 4.0638\nVal Loss: 5.3434, BLEU Score: 0.0022\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|██████████| 30/30 [01:31<00:00,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 30/30:\nTrain Loss: 3.7024\nVal Loss: 5.2972, BLEU Score: 0.0029\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"model.eval()\npred_captions = []\nwith torch.no_grad():\n    for embeddings, _ in test_loader:\n        embeddings = embeddings.to(device)\n        generated_ids = model.generate(embeddings, tokenizer, max_len=50, device=device)\n        for ids in generated_ids:\n            caption = tokenizer.decode(ids, skip_special_tokens=True)\n            pred_captions.append(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:36:43.868651Z","iopub.execute_input":"2025-06-23T09:36:43.869190Z","iopub.status.idle":"2025-06-23T09:36:46.196306Z","shell.execute_reply.started":"2025-06-23T09:36:43.869165Z","shell.execute_reply":"2025-06-23T09:36:46.195773Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"submission = pd.DataFrame(\n    {\n        \"index\": test_df.index,\n        \"file_name\": test_df[\"file_name\"],\n        \"caption\": pred_captions,\n    }\n)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:36:48.792276Z","iopub.execute_input":"2025-06-23T09:36:48.792870Z","iopub.status.idle":"2025-06-23T09:36:48.802081Z","shell.execute_reply.started":"2025-06-23T09:36:48.792845Z","shell.execute_reply":"2025-06-23T09:36:48.801541Z"}},"outputs":[],"execution_count":39}]}